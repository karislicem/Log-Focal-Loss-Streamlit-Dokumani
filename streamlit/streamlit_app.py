import streamlit as st
import numpy as np
import matplotlib.pyplot as plt

st.title("ğŸ“‰ Log Loss ve Focal Loss Nedir?")

st.markdown("""
SÄ±nÄ±flandÄ±rma problemlerinde modelin tahmin baÅŸarÄ±sÄ±nÄ± deÄŸerlendirmek iÃ§in 
<span style='color:#ff4b4b'><b>Log Loss</b></span> ve 
<span style='color:#ff4b4b'><b>Focal Loss</b></span> gibi loss'lar kullanÄ±lÄ±r.  

- <b>Log Loss</b>, modelin tahmin ettiÄŸi olasÄ±lÄ±klar Ã¼zerinden hesaplanÄ±r ve 
  yanlÄ±ÅŸ ama emin tahminlere yÃ¼ksek ceza verir.
- <b>Focal Loss</b> ise Ã¶zellikle dengesiz veri setlerinde,
  modelin zor Ã¶rneklere daha fazla odaklanmasÄ±nÄ± saÄŸlar.

Bu uygulamada bu iki loss'un nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± formÃ¼ller, grafikler ve kÄ±sa aÃ§Ä±klamalarla inceliyoruz.
""", unsafe_allow_html=True)

st.header("ğŸ”· Log Loss")

st.latex(r"""
\text{Log Loss} = -[y \cdot \log(p) + (1 - y) \cdot \log(1 - p)]
""")

st.markdown("""
- <b>y</b>: GerÃ§ek sÄ±nÄ±f etiketi (0 veya 1)  
- <b>p</b>: Modelin 1 sÄ±nÄ±fÄ± iÃ§in tahmin ettiÄŸi olasÄ±lÄ±k  
Log Loss; modelin ne kadar "emin olup hata yaptÄ±ÄŸÄ±nÄ±" Ã¶lÃ§er.
""", unsafe_allow_html=True)

with st.expander("ğŸ’¡ Neden log kullanÄ±yoruz?"):
    st.markdown("""
    - Logaritmik yapÄ±; kÃ¼Ã§Ã¼k hatalara dÃ¼ÅŸÃ¼k, bÃ¼yÃ¼k hatalara yÃ¼ksek ceza verir.  
    - Model Ã§ok dÃ¼ÅŸÃ¼k olasÄ±lÄ±k verip yanÄ±lÄ±rsa (Ã¶rneÄŸin `y=1`, `p=0.01`), ceza Ã§ok bÃ¼yÃ¼r.  
    - Bu, modeli daha dikkatli tahminler yapmaya teÅŸvik eder.  
    - Bu yÃ¼zden sÄ±nÄ±flandÄ±rmalarda en yaygÄ±n kullanÄ±lan loss'tur.
    """)

p = np.linspace(0.001, 0.999, 100)
loss_1 = -np.log(p)
loss_0 = -np.log(1 - p)

fig1, ax1 = plt.subplots()
ax1.plot(p, loss_1, label='GerÃ§ek: y = 1')
ax1.plot(p, loss_0, label='GerÃ§ek: y = 0')
ax1.set_title("Log Loss")
ax1.set_xlabel("Modelin tahmin ettiÄŸi olasÄ±lÄ±k (p)")
ax1.set_ylabel("Loss")
ax1.grid(True)
ax1.legend()
st.pyplot(fig1)

st.header("ğŸ”· Focal Loss")

st.latex(r"""
\text{FL}(p_t) = -\alpha \cdot (1 - p_t)^\gamma \cdot \log(p_t)
""")

st.markdown("""
- <b>pâ‚œ</b>: Modelin doÄŸru sÄ±nÄ±fa verdiÄŸi olasÄ±lÄ±k  
- <b>Î³ (gamma)</b>: Odaklanma parametresi (zor Ã¶rneklerin aÄŸÄ±rlÄ±ÄŸÄ±nÄ± artÄ±rÄ±r)  
- <b>Î± (alpha)</b>: SÄ±nÄ±f aÄŸÄ±rlÄ±ÄŸÄ± (opsiyonel)  

Focal Loss, kolay Ã¶rneklerin etkisini azaltÄ±r. BÃ¶ylece model, zor Ã¶rnekleri daha fazla Ã¶ÄŸrenmeye Ã§alÄ±ÅŸÄ±r.
""", unsafe_allow_html=True)

with st.expander("ğŸ’¡ Focal Loss ne zaman tercih edilir?"):
    st.markdown("""
    - EÄŸer veri setin dengesizse (%90 negatif, %10 pozitif gibi), Log Loss Ã§oÄŸunluk sÄ±nÄ±fa odaklanÄ±r.  
    - Bu durumda azÄ±nlÄ±k gÃ¶rÃ¼len sÄ±nÄ±flar ihmal edilebilir.  
    - Focal Loss bu azÄ±nlÄ±k sÄ±nÄ±flara daha fazla odaklanma imkanÄ± verir.  
    - Ã–zellikle <b>nesne tespiti</b>, <b>fraud tespiti</b> ve <b>medikal veri</b> gibi alanlarda sÄ±k kullanÄ±lÄ±r.
    """, unsafe_allow_html=True)

gamma = st.slider("Gamma (odaklanma gÃ¼cÃ¼)", 0.0, 5.0, 2.0, 0.1)
alpha = st.slider("Alpha (sÄ±nÄ±f aÄŸÄ±rlÄ±ÄŸÄ±)", 0.0, 1.0, 1.0, 0.1)

pt = np.linspace(0.001, 0.999, 100)
focal_loss = -alpha * (1 - pt)**gamma * np.log(pt)

fig2, ax2 = plt.subplots()
ax2.plot(pt, focal_loss, label=f'Î³ = {gamma}, Î± = {alpha}')
ax2.set_title("Focal Loss (GerÃ§ek sÄ±nÄ±f = 1)")
ax2.set_xlabel("Modelin doÄŸru sÄ±nÄ±fa verdiÄŸi tahmin olasÄ±lÄ±ÄŸÄ± (pâ‚œ)")
ax2.set_ylabel("Loss")
ax2.grid(True)
ax2.legend()
st.pyplot(fig2)

st.header("ğŸ“Œ Ã–zet")

st.markdown("""
- <b>Log Loss</b>: Her Ã¶rneÄŸi eÅŸit Ã¶nemser. HatalÄ± ve emin tahminlerde Ã§ok ceza verir.  
- <b>Focal Loss</b>: Kolay Ã¶rneklerin etkisini azaltÄ±r, zor Ã¶rnekleri Ã¶ne Ã§Ä±karÄ±r.  
- Dengesiz veri setlerinde Focal Loss tercih edilirse, model daha adil sonuÃ§lar verebilir.
""", unsafe_allow_html=True)

with st.expander("ğŸ“š Ä°lgili Makaleler ve Grafikler"):
    st.markdown("### ğŸ“˜ 1. *Focal Loss for Dense Object Detection*")
    st.markdown("""
**Yazarlar:** Lin, Goyal, Girshick, He, & DollÃ¡r (2017)  
**Ã–zet:** One-stage object detection modellerinde sÄ±nÄ±f dengesizliÄŸini Ã§Ã¶zmek iÃ§in Focal Loss Ã¶nerildi.  
Yeni geliÅŸtirilen RetinaNet modeliyle baÅŸarÄ± elde edildi.
    """)

    st.markdown("### ğŸ“˜ 2. *Calibrating Deep Neural Networks using Focal Loss*")
    st.markdown("""
**Yazarlar:** Mukhoti, Ghosh, Kumar, & Torr (2020)  
**Ã–zet:** Focal Lossâ€™un aÄŸlarÄ±n gÃ¼ven kalibrasyonunu da iyileÅŸtirdiÄŸi gÃ¶sterildi.  
Tahmin edilen olasÄ±lÄ±klar, gerÃ§ek olasÄ±lÄ±klara daha yakÄ±n hale geliyor.
    """)

    st.markdown("### ğŸ“˜ 3. *A Comprehensive Review of Loss Functions and Metrics in Deep Learning Classification*")
    st.markdown("""
**Yazarlar:** Terven & Cordova-Esparza (2021)  
**Ã–zet:** TÃ¼m popÃ¼ler loss ve metrikler karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±. Log Loss ve Focal Loss'un avantajlarÄ± tablolarla aÃ§Ä±klandÄ±.
    """)

    st.markdown("### ğŸ“Š FarklÄ± Gamma DeÄŸerleriyle Focal Loss GrafiÄŸi")

    gamma_vals = [0, 1, 2, 5]
    fig3, ax3 = plt.subplots()
    for g in gamma_vals:
        fl = -(1 - pt)**g * np.log(pt)
        ax3.plot(pt, fl, label=f"Î³={g}")
    ax3.set_title("Gamma Parametresine GÃ¶re Focal Loss DeÄŸiÅŸimi (Î±=1)")
    ax3.set_xlabel("pâ‚œ")
    ax3.set_ylabel("Loss")
    ax3.legend()
    ax3.grid(True)
    st.pyplot(fig3)

    st.markdown("### ğŸ“– APA FormatÄ±nda Kaynaklar")
    st.markdown("""
**1.** Lin, T.-Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017).  
Focal loss for dense object detection. *ICCV*, 2980â€“2988. https://doi.org/10.1109/ICCV.2017.324

**2.** Mukhoti, J., Ghosh, A., Kumar, M. P., & Torr, P. H. S. (2020).  
Calibrating deep neural networks using focal loss. *NeurIPS*, 33, 15288â€“15299. https://arxiv.org/abs/2002.09437

**3.** Terven, J., & Cordova-Esparza, D. M. (2021).  
A comprehensive review of loss functions and metrics for deep learning classification. *Pattern Recognition*, 125, 108198. https://doi.org/10.1016/j.patcog.2021.108198
    """)

st.sidebar.markdown("ğŸ‘¤ Cem KarÄ±ÅŸlÄ±  \n[GitHub](https://github.com/karislicem) | [LinkedIn](https://www.linkedin.com/in/karislicem/)")
